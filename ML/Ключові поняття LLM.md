
Токени - мінімально одиниця тексту. Моделі працюють не з текстом, а з числами. Тому кожен токен перетвоюється в унікальне число. 
Наприклад - "Hello world!" -> ["Hello", ",", " ", "world", "!"] -> [1234, 5678, 452, 9101, 1121]

Як відбувається токенізація
- Попередня обробка тексту - видалення пробілів, перенесення рядків, нормалізаця реєстру, заміна -- на -
- Розбиття на токени - використовується токенізаціто і його словник, текст розбивається на послідовність токенів (50 тисяч токенів у GPT-2)
- Кодування в токен-ID - кожному токену присвоюється унікальний ID

Підходи до токенізації
- **Word-based tokenization** (по словах) - текст ділиться слова як цілі одиниці. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собака", " ", "гавкає", ",", " ", "кішка", " ", "ховається", "."]. Переваги - просто реалізувати. Але з недоліків має бути передбачено різні орфографічні варіанти одного слова.
- **Character-based tokenization (за символами)** - текст поділяється на окремі символи. Вихідний текст: "Собака". Токенізація: ["С", "о", "б", "а", "к", "а"]. Переваги - будь яке слово може бути закодоване, але це призводить до дуже довгих послідовностей. 
- **Subword tokenization (підмовна токенізація)** - текст ділиться на підслів - послідовності букв, які часто зустрічаються разом. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собак", "а", "гавк", "ає", ",", "кішка", "а", "хова", "ється", "."]. 

Архітектура

**Transformer** в режимі decoder-only - модель вчиться передбачати наступний токен послідовності на основі всіх попередніх, тобто грають у _«вгадай наступне слово»_. Цей підхід відомий як **каузальне мовне моделювання** : модель дивиться на все, що ви написали до цього моменту, і намагається передбачити, що буде далі. Якби ви писали повідомлення, а нейромережа щохвилини підштовхувала: «Далі буде... „зустріч“», «А тепер — „у п'ятницю“».
P(xₜ₊₁ | x₁, x₂, ..., xₜ)
Тут модель оцінює умовний розподіл ймовірностей для наступного токена xₜ₊₁ , використовуючи інформацію з усієї попередньої послідовності x₁, x₂, ..., xₜ.
Для цього застосовується **механізм уваги** , нейромережа вміє «підсвічувати» важливі частини тексту:

**Функція втрат**

## Ембеддінги

Ембеддинг – це числовий вектор, який представляє певний дискретний об'єкт (наприклад, слово, речення чи абзац) у багатовимірному просторі. Головна ідея ембеддингів полягає в тому, щоб розташувати близькі за змістом елементи менш віддалено один від одного в цьому просторі. 
Наприклад, слова "король" і "цар" повинні мати близькі ембеддінги, тоді як "король" і "банан" - бути далі один від одного.

**Як створюються ембеддінги в LLM?**
На етапі вхідного кодування текст перетворюється на послідовність токенів  $t1​,t2​,…,tn​$, які потім замінюються на відповідні вектори з матриці ембеддингів.
Ембеддінги не задають вручну - **модель вчить їх сама** в процесі тренування. Мета - мінімізація функції втрат, найчастіше перехресної ентропії. Спочатку модель плутатиме «кіт» і «кіт», потім зауважує, що «кіт» частіше поряд з «мишею», а «кит» — з «океаном» і розводить їх вектори у різні боки.
Перехресна ентропія – це спосіб **оцінити, наскільки добре передбачення моделі збігаються з реальністю** .

Вона **штрафує модель** , якщо вона впевнено помиляється.

Якщо модель невпевнена, але в правильному напрямку штраф менший.

Ідеальне передбачення (100% вірогідність правильного класу) отримує **мінімальний штраф** .

**Проміжні ембеддінги**

Однією з найдивовижніших властивостей ембеддингів є можливість виконувати арифметичні операції над векторами, що відображають логічні та семантичні відносини між словами. Наприклад:

_король - чоловік + жінка - королева_

Це стало можливо завдяки навчанню на величезних корпусах тексту, де модель сама знаходить приховані залежності між словами та їх контекстами. Такі закономірності дозволяють використовувати ембеддинги як для генерації тексту, а й у вирішення завдань перекладу, аналізу тональності, рекомендацій та інших завдань NLP без явного зазначення цих цілей під час навчання.

## Температура

Температура належить до параметра генерації тексту, який впливає ступінь випадковості чи детермінованості під час виборів наступного слова у згенерованому тексті. Параметр температури перетворює нейромережі із занудних статистиків на креативних письменників.
Формула зображена так:

$Psoftmax(xi)=ezi/T∑jezj/TP_{softmax}(x_i) = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$

Текстом:

$Psoftmax(xᵢ) = exp(zᵢ / T) / ∑ⱼ exp(zⱼ / T)$

Де:

- **zᵢ** — логіт (сирий висновок моделі для **i**-го токена),
    
- **T** — температура (параметр, що впливає на “розмитість” розподілу),
    
- **Psoftmax(xᵢ)** — скоригована ймовірність вибору токена **xᵢ** після застосування softmax з температурою.
    

### Пояснення:

- Якщо **T = 1**, це звичайний softmax.
    
- Якщо **T < 1**, вибір стає більш “впевненим” — великі логіти ще більше переважають.
    
- Якщо **T > 1**, вибір стає менш впевненим — розподіл стає більш пласким.

