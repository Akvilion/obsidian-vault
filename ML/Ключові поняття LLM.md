
Токени - мінімально одиниця тексту. Моделі працюють не з текстом, а з числами. Тому кожен токен перетвоюється в унікальне число. 
Наприклад - "Hello world!" -> ["Hello", ",", " ", "world", "!"] -> [1234, 5678, 452, 9101, 1121]

Як відбувається токенізація
- Попередня обробка тексту - видалення пробілів, перенесення рядків, нормалізаця реєстру, заміна -- на -
- Розбиття на токени - використовується токенізаціто і його словник, текст розбивається на послідовність токенів (50 тисяч токенів у GPT-2)
- Кодування в токен-ID - кожному токену присвоюється унікальний ID

Підходи до токенізації
- **Word-based tokenization** (по словах) - текст ділиться слова як цілі одиниці. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собака", " ", "гавкає", ",", " ", "кішка", " ", "ховається", "."]. Переваги - просто реалізувати. Але з недоліків має бути передбачено різні орфографічні варіанти одного слова.
- **Character-based tokenization (за символами)** - текст поділяється на окремі символи. Вихідний текст: "Собака". Токенізація: ["С", "о", "б", "а", "к", "а"]. Переваги - будь яке слово може бути закодоване, але це призводить до дуже довгих послідовностей. 
- **Subword tokenization (підмовна токенізація)** - текст ділиться на підслів - послідовності букв, які часто зустрічаються разом. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собак", "а", "гавк", "ає", ",", "кішка", "а", "хова", "ється", "."]. 

Архітектура

**Transformer** в режимі decoder-only - модель вчиться передбачати наступний токен послідовності на основі всіх попередніх, тобто грають у _«вгадай наступне слово»_. Цей підхід відомий як **каузальне мовне моделювання** : модель дивиться на все, що ви написали до цього моменту, і намагається передбачити, що буде далі. Якби ви писали повідомлення, а нейромережа щохвилини підштовхувала: «Далі буде... „зустріч“», «А тепер — „у п'ятницю“».
P(xₜ₊₁ | x₁, x₂, ..., xₜ)
Тут модель оцінює умовний розподіл ймовірностей для наступного токена xₜ₊₁ , використовуючи інформацію з усієї попередньої послідовності x₁, x₂, ..., xₜ.
Для цього застосовується **механізм уваги** , нейромережа вміє «підсвічувати» важливі частини тексту:

