
Токени - мінімально одиниця тексту. Моделі працюють не з текстом, а з числами. Тому кожен токен перетвоюється в унікальне число. 
Наприклад - "Hello world!" -> ["Hello", ",", " ", "world", "!"] -> [1234, 5678, 452, 9101, 1121]

Як відбувається токенізація
- Попередня обробка тексту - видалення пробілів, перенесення рядків, нормалізаця реєстру, заміна -- на -
- Розбиття на токени - використовується токенізаціто і його словник, текст розбивається на послідовність токенів (50 тисяч токенів у GPT-2)
- Кодування в токен-ID - кожному токену присвоюється унікальний ID

Підходи до токенізації
- **Word-based tokenization** (по словах) - текст ділиться слова як цілі одиниці. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собака", " ", "гавкає", ",", " ", "кішка", " ", "ховається", "."]. Переваги - просто реалізувати. Але з недоліків має бути передбачено різні орфографічні варіанти одного слова.
- **Character-based tokenization (за символами)** - текст поділяється на окремі символи. Вихідний текст: "Собака". Токенізація: ["С", "о", "б", "а", "к", "а"]. Переваги - будь яке слово може бути закодоване, але це призводить до дуже довгих послідовностей. 
- **Subword tokenization (підмовна токенізація)** - текст ділиться на підслів - послідовності букв, які часто зустрічаються разом. Вихідний текст: «Собака гавкає, кішка ховається». Токенізація: ["Собак", "а", "гавк", "ає", ",", "кішка", "а", "хова", "ється", "."]. 